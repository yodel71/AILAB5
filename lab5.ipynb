{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import math\n",
    "import chardet\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(values):\n",
    "    dataset = []\n",
    "    for i in range(len(values)):\n",
    "        guid = str(int(values[i][0]))\n",
    "        label = values[i][1]\n",
    "        if type(label) != str and math.isnan(label):\n",
    "            label = None\n",
    "        file = path_text + guid + '.txt'\n",
    "        with open(file, 'rb') as f:\n",
    "            encoding = chardet.detect(f.read())['encoding']\n",
    "            if encoding == \"GB2312\":\n",
    "                encoding = \"GBK\"\n",
    "        text = ''\n",
    "        try:\n",
    "            with open(file, encoding=encoding) as fp:\n",
    "                for line in fp.readlines():\n",
    "                    line = line.strip('\\n')\n",
    "                    text += line\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                with open(file, encoding='ANSI') as fp:\n",
    "                    for line in fp.readlines():\n",
    "                        line = line.strip('\\n')\n",
    "                        text += line\n",
    "            except UnicodeDecodeError:\n",
    "                print('UnicodeDecodeError')\n",
    "        dataset.append({\n",
    "            'text': text,\n",
    "            'label': label,\n",
    "            'img': path_text + guid + '.jpg',\n",
    "        })\n",
    "    return dataset\n",
    "\n",
    "path_train = 'train.txt'\n",
    "path_test = 'test_without_label.txt'\n",
    "path_text = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写入到json文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_val = pd.read_csv(path_train)\n",
    "test_data = pd.read_csv(path_test)\n",
    "# 划分训练集、验证集\n",
    "train_data, val_data = train_test_split(train_data_val, test_size=0.2)\n",
    "train_set = get_dataset(train_data.values)\n",
    "val_set = get_dataset(val_data.values)\n",
    "test_set = get_dataset(test_data.values)\n",
    "\n",
    "with open('data_json/train.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(train_set, f)\n",
    "\n",
    "with open('data_json/val.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(val_set, f)\n",
    "\n",
    "with open('data_json/test.json', \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_json/train.json','r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('data_json/val.json','r') as f:\n",
    "    val_data = json.load(f)\n",
    "with open('data_json/test.json','r') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_list(data, flag):\n",
    "    text_or_label = 'text' if flag == 1 else 'label'\n",
    "    return [item[text_or_label] for item in data]\n",
    "\n",
    "# text 和 label 分别存储\n",
    "train_text_list = process_list(train_data, 1)\n",
    "val_text_list = process_list(val_data, 1)\n",
    "train_labels = process_list(train_data, 0)\n",
    "val_labels = process_list(val_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter() \n",
    "i=0\n",
    "for text in train_text_list:\n",
    "    words_list = nltk.word_tokenize(text) \n",
    "    words.update(words_list)  \n",
    "    train_text_list[i] = words_list\n",
    "    i+=1\n",
    "\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "words = sorted(words, key=words.get,reverse=True)\n",
    "words = ['_PAD'] + words\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for text in train_text_list:  \n",
    "    train_text_list[i] = [word2idx[word] if word in word2idx else 0 for word in text]\n",
    "    i+=1\n",
    "\n",
    "i=0\n",
    "for text in val_text_list:\n",
    "    val_text_list[i] = [word2idx[word] if word in word2idx else 0 for word in nltk.word_tokenize(text)]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(text_list, seq_len):\n",
    "    features = np.zeros((len(text_list), seq_len),dtype=int)\n",
    "    i=0\n",
    "    for text in text_list:\n",
    "        features[i, -len(text):] = np.array(text)[:seq_len]\n",
    "        i+=1\n",
    "    return features\n",
    "\n",
    "train_text = padding(train_text_list, 200)\n",
    "val_text = padding(val_text_list, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(data_labels):\n",
    "  i = 0\n",
    "  for label in data_labels:\n",
    "    if(data_labels[i]=='negative'):\n",
    "      data_labels[i]=0\n",
    "    elif(data_labels[i]=='positive'):\n",
    "        data_labels[i]=1\n",
    "    elif(data_labels[i]=='neutral'):\n",
    "        data_labels[i]=2\n",
    "    i+=1\n",
    "  return data_labels\n",
    "\n",
    "train_labels = np.array(change_label(train_labels))\n",
    "val_labels = np.array(change_label(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "消融实验：仅文本训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_Data = TensorDataset(torch.from_numpy(train_text), torch.from_numpy(train_labels))\n",
    "val_Data = TensorDataset(torch.from_numpy(val_text), torch.from_numpy(val_labels))\n",
    "\n",
    "train_loader = DataLoader(train_Data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_Data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMaxPool1d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalMaxPool1d, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return F.max_pool1d(x, kernel_size=x.shape[2]) \n",
    "    \n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        self.constant_embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        self.pool = GlobalMaxPool1d()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(in_channels = 2*embed_size, \n",
    "                          out_channels = c, \n",
    "                          kernel_size = k))\n",
    "        self.decoder = nn.Linear(sum(num_channels), 3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), \n",
    "            self.constant_embedding(inputs)), dim=2)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        encoding = torch.cat([\n",
    "            self.pool(F.relu(conv(embeddings))).squeeze(-1) for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs\n",
    "\n",
    "embed_size, kernel_sizes, nums_channels = 200, [2, 3, 4], [100, 100, 100]\n",
    "net = TextCNN(words, embed_size, kernel_sizes, nums_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer,  num_epochs):\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y.to(torch.int64))\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
    "\n",
    "def evaluate_accuracy(data_iter, net, device=None):\n",
    "    if device is None and isinstance(net, torch.nn.Module):\n",
    "        device = list(net.parameters())[0].device \n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(net, torch.nn.Module):\n",
    "                net.eval()\n",
    "                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "                net.train()\n",
    "            else:\n",
    "                if('is_training' in net.__code__.co_varnames):\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-only training start\n",
      "epoch 1, loss 0.9825, train acc 0.556, test acc 0.633, time 26.9 sec\n",
      "epoch 2, loss 0.3883, train acc 0.660, test acc 0.649, time 39.2 sec\n",
      "epoch 3, loss 0.2196, train acc 0.731, test acc 0.660, time 38.1 sec\n",
      "epoch 4, loss 0.1417, train acc 0.785, test acc 0.670, time 26.3 sec\n",
      "epoch 5, loss 0.0968, train acc 0.841, test acc 0.676, time 25.3 sec\n",
      "epoch 6, loss 0.0667, train acc 0.880, test acc 0.687, time 28.6 sec\n",
      "epoch 7, loss 0.0478, train acc 0.903, test acc 0.694, time 44.2 sec\n",
      "epoch 8, loss 0.0340, train acc 0.937, test acc 0.705, time 34.3 sec\n",
      "epoch 9, loss 0.0252, train acc 0.948, test acc 0.707, time 30.9 sec\n",
      "epoch 10, loss 0.0192, train acc 0.954, test acc 0.711, time 42.8 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0005\n",
    "num_epochs = 10\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "print(\"text-only training start\")\n",
    "train(train_loader, val_loader, net, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "消融实验：仅图片训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToDataset(Dataset):\n",
    "    def __init__(self,main_dir):\n",
    "        self.dataset=[]\n",
    "        i=0\n",
    "        for data in main_dir:\n",
    "          self.dataset.append([data['img'],data['label'],i])\n",
    "          i+=1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img,label,i=self.dataset[index]\n",
    "        img_data=self.data_process(Image.open(img))\n",
    "        if(label=='negative'):\n",
    "          label = 0\n",
    "        elif(label=='positive'):\n",
    "          label = 1\n",
    "        elif(label=='neutral'):\n",
    "          label = 2\n",
    "        elif(label is None):\n",
    "          label = -1\n",
    "        return img_data,label\n",
    "\n",
    "    def data_process(self,x):\n",
    "        return transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((256,256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.5,0.5,0.5],\n",
    "                std=[0.5,0.5,0.5],\n",
    "            ),\n",
    "        ]\n",
    "    )(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=ToDataset(train_data), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=ToDataset(val_data), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=ToDataset(test_data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,   \n",
    "                out_channels=16, \n",
    "                kernel_size=5,  \n",
    "                stride=1, \n",
    "                padding=2, \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,   \n",
    "                out_channels=32,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4) \n",
    "        )\n",
    "        self.output = nn.Linear(in_features=32*16*16, out_features=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "photo-only training start\n",
      "epoch 1, loss 0.9144, train acc 0.591, test acc 0.599, time 246.8 sec\n",
      "epoch 2, loss 0.4454, train acc 0.597, test acc 0.599, time 180.9 sec\n",
      "epoch 3, loss 0.2936, train acc 0.597, test acc 0.596, time 191.8 sec\n",
      "epoch 4, loss 0.2179, train acc 0.600, test acc 0.599, time 188.0 sec\n",
      "epoch 5, loss 0.1728, train acc 0.602, test acc 0.600, time 179.0 sec\n",
      "epoch 6, loss 0.1414, train acc 0.603, test acc 0.604, time 154.6 sec\n",
      "epoch 7, loss 0.1193, train acc 0.611, test acc 0.607, time 146.9 sec\n",
      "epoch 8, loss 0.1016, train acc 0.635, test acc 0.609, time 147.3 sec\n",
      "epoch 9, loss 0.0882, train acc 0.633, test acc 0.611, time 137.0 sec\n",
      "epoch 10, loss 0.0758, train acc 0.654, test acc 0.615, time 132.9 sec\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=lr)\n",
    "print(\"photo-only training start\")\n",
    "train(train_loader, val_loader, cnn, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多模态融合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiModel(data,data_loader):\n",
    "  def predictText(net, vocab, sentence):\n",
    "      device = list(net.parameters())[0].device\n",
    "      sentence = torch.tensor([word2idx[word] if word in word2idx else 0 for word in sentence], device=device)\n",
    "      label = torch.argmax(net(sentence.view((1, -1))), dim=1)\n",
    "      if(label.item()==0):\n",
    "        return 'negative'\n",
    "      elif(label.item()==1):\n",
    "        return 'positive'\n",
    "      else:\n",
    "        return 'neutral'\n",
    "      \n",
    "  predict_text=[]\n",
    "  for i in range(len(data)):\n",
    "    if(len(nltk.word_tokenize(data[i]['text']))>=4):\n",
    "      predict_text.append(predictText(net, words, nltk.word_tokenize(data[i]['text'])))\n",
    "    else:\n",
    "      predict_text.append('neutral')\n",
    "\n",
    "  predict=[]\n",
    "  with torch.no_grad():\n",
    "      for X, y in data_loader:\n",
    "          if isinstance(cnn, torch.nn.Module):\n",
    "              predict.append(cnn(X).argmax(dim=1))\n",
    "\n",
    "  predict_pic=[]\n",
    "  for batch in predict:\n",
    "    for i in batch:\n",
    "      if(i==0):\n",
    "        predict_pic.append('negative')\n",
    "      elif(i==1):\n",
    "        predict_pic.append('positive')\n",
    "      elif(i==2):\n",
    "        predict_pic.append('neutral')\n",
    "\n",
    "  predict_final=[]\n",
    "  for i in range(len(data)):\n",
    "    if(predict_text[i]=='positive' and predict_pic[i]=='positive'):\n",
    "      predict_final.append('positive')\n",
    "    elif(predict_text[i]=='positive' and predict_pic[i]=='neutral'):\n",
    "      predict_final.append('positive')\n",
    "    elif(predict_text[i]=='positive' and predict_pic[i]=='negative'):\n",
    "      predict_final.append('positive')\n",
    "    elif(predict_text[i]=='negative' and predict_pic[i]=='positive'):\n",
    "      predict_final.append('negative')\n",
    "    elif(predict_text[i]=='negative' and predict_pic[i]=='neutral'):\n",
    "      predict_final.append('negative')\n",
    "    elif(predict_text[i]=='negative' and predict_pic[i]=='negative'):\n",
    "      predict_final.append('negative')\n",
    "    elif(predict_text[i]=='neutral' and predict_pic[i]=='positive'):\n",
    "      predict_final.append('positive')\n",
    "    elif(predict_text[i]=='neutral' and predict_pic[i]=='negative'):\n",
    "      predict_final.append('negative')\n",
    "    else:\n",
    "      predict_final.append('neutral')\n",
    "\n",
    "  return predict_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "val_predict = multiModel(val_data,val_loader)\n",
    "acc_count = 0\n",
    "for i in range(len(val_data)):\n",
    "  if val_predict[i] == val_data[i][\"label\"]:\n",
    "    acc_count += 1\n",
    "print(acc_count/len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = multiModel(test_data,test_loader)\n",
    "test_data_file = pd.read_csv(\"test_without_label.txt\")['guid'].values\n",
    "with open('test_without_label.txt','w') as f:\n",
    "  f.write('guid,tag\\n')\n",
    "  for i in range(len(test_data_file)):\n",
    "    f.write(str(test_data_file[i])+','+str(test_predict[i])+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
